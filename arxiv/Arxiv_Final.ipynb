{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9DyU5HC-5dk"
      },
      "source": [
        "TP Bensafi Sarra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYJApfkfD8qm"
      },
      "source": [
        "Our dataset is ogbn-arxiv network in which each node is a Computer Science paper on the arxiv and each directed edge represents that a paper cited another. The task is to classify each node into a paper class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEyuUvwiMUqW"
      },
      "outputs": [],
      "source": [
        "#INSTALLATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtwqu1TLIVOl",
        "outputId": "a9cfc24a-779c-4966-c442-d02d82c9486b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.12.0+cpu.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcpu/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (286 kB)\n",
            "\u001b[K     |████████████████████████████████| 286 kB 2.7 MB/s \n",
            "\u001b[?25hCollecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcpu/torch_sparse-0.6.15-cp37-cp37m-linux_x86_64.whl (641 kB)\n",
            "\u001b[K     |████████████████████████████████| 641 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcpu/torch_cluster-1.6.0-cp37-cp37m-linux_x86_64.whl (311 kB)\n",
            "\u001b[K     |████████████████████████████████| 311 kB 3.9 MB/s \n",
            "\u001b[?25hCollecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcpu/torch_spline_conv-1.2.1-cp37-cp37m-linux_x86_64.whl (121 kB)\n",
            "\u001b[K     |████████████████████████████████| 121 kB 11.4 MB/s \n",
            "\u001b[?25hCollecting torch-geometric\n",
            "  Downloading torch_geometric-2.1.0.post1.tar.gz (467 kB)\n",
            "\u001b[K     |████████████████████████████████| 467 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.7.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2022.9.24)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.1.0.post1-py3-none-any.whl size=689859 sha256=85cf75a63ebca98f7e8364502455b5f4d1b9db82b22589f9efffcc1f69f7808c\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/cb/43/f7f2e472de4d7cff31bceddadc36d634e1e545fbc17961c282\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-spline-conv, torch-sparse, torch-scatter, torch-geometric, torch-cluster\n",
            "Successfully installed torch-cluster-1.6.0 torch-geometric-2.1.0.post1 torch-scatter-2.0.9 torch-sparse-0.6.15 torch-spline-conv-1.2.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n",
            "Python 3.7.15\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.4-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (4.64.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.12.1+cu113)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.24.3)\n",
            "Collecting outdated>=0.2.0\n",
            "  Downloading outdated-0.2.1-py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.0.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.3.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.15.0)\n",
            "Collecting littleutils\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2022.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->ogb) (4.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7048 sha256=361bb6bc1b2ff3b24c9946c78f771bddc49c85954a75fd03c4d0aa3e3b1297e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/64/cd/32819b511a488e4993f2fab909a95330289c3f4e0f6ef4676d\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.4 outdated-0.2.1\n"
          ]
        }
      ],
      "source": [
        "#Imports\n",
        "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.12.0+cpu.html\n",
        "!pip install -U scikit-learn\n",
        "!python --version\n",
        "!pip install -U ogb\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch_geometric\n",
        "import urllib3\n",
        "import outdated\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oqxc20UzMfYz"
      },
      "source": [
        "### Informations about graphs\n",
        "if the graphs are complex we need :\n",
        "  Adjacency matrix: defines how the nodes are connected to each other in a n by n matrix,\n",
        "    \n",
        "  Edge attributes: the value of edge (eg. the distance in meters )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHtHb2PIIryo"
      },
      "outputs": [],
      "source": [
        "#\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "\n",
        "    \n",
        "    def __init__(self, data, num_classes_end, hidden=256\n",
        "                 , layers_gcn=4):\n",
        "\n",
        "        super(GCN, self).__init__()#super it self\n",
        "\n",
        "        self.num_layers_gcn = layers_gcn\n",
        "        self.conv1 = GCNConv(data.num_node_features, hidden)#num_node_features  grand ici 128\n",
        "        #The final output of GCN ais a matrix Z , and the shape is N*F . \n",
        "          #N is the number of nodes, \n",
        "          #F is the number of output features per node\n",
        "\n",
        "        #The hidden channels parameter that represents the number of hidden units.\n",
        "        self.bn1=torch.nn.BatchNorm1d(hidden)         \n",
        "        #we apply BatchNorm1d to the node feature matrix. This way, you compute \n",
        "        #statistics for each feature across all nodes in the batch\n",
        "\n",
        "        self.conv2= GCNConv(hidden, hidden) #SAGEConv(hidden, hidden,   normalize=False)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(hidden)\n",
        "        self.conv3= GCNConv(hidden, hidden) \n",
        "        self.bn3 = torch.nn.BatchNorm1d(hidden)\n",
        "\n",
        "        # couche de sortie\n",
        "        self.conv4 = GCNConv(hidden, num_classes_end)\n",
        "\n",
        "    # reset les paramètres \n",
        "    def reset_parameters(self):\n",
        "        \n",
        "      self.conv1.reset_parameters()\n",
        "      self.conv2.reset_parameters()\n",
        "      self.conv3.reset_parameters()\n",
        "      self.conv4.reset_parameters()\n",
        "      self.bn1.reset_parameters()\n",
        "      self.bn2.reset_parameters()\n",
        "      self.bn3.reset_parameters()\n",
        "      \n",
        "\n",
        "    def forward(self, data):\n",
        "        # Nos données d'entrée \n",
        "         # x : les valeurs des  nœuds\n",
        "         # les nœuds d'origine et de destinations\n",
        "        x, adj_t = data.x, data.adj_t\n",
        "        # on ajoute les couches de convolution\n",
        "        x = self.conv1(x, adj_t)  #initialiser la première couche convolutive de graphe,  \n",
        "        x = self.bn1(x) \n",
        "        #ReLU and a dropout layer for regularization purposes\n",
        "        x = F.relu(x)#suivie d'une fonction d'activation ReLU\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        \n",
        "        x = self.conv2(x, adj_t)\n",
        "        x = self.bn2(x) \n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv3(x, adj_t)\n",
        "        x = self.bn3(x) \n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        \n",
        "        # couche de sortie\n",
        "        x = self.conv4(x, adj_t)\n",
        "        \n",
        "        x = F.log_softmax(x, dim=1)  \n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGdARskNOH9N"
      },
      "outputs": [],
      "source": [
        "def train(model, data, train_idx, optimizer):\n",
        "    model.train() #entrainer le models\n",
        "    optimizer.zero_grad() #In PyTorch, for every mini-batch during the training phase, we typically want to explicitly set the gradients to zero before starting to do backpropragation (i.e., updating the Weights and biases) because PyTorch accumulates the gradients on subsequent backward passes. This accumulating behaviour is convenient while training RNNs or when we want to compute the gradient of the loss summed over multiple mini-batches. So, the default action has been set to accumulate (i.e. sum) the gradients on every loss.backward() call.\n",
        "    \n",
        "    out = model(data)[train_idx]\n",
        "    loss = F.nll_loss(out, data.y.squeeze(1)[train_idx])#negative log likelihood\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, data, split_idx, evaluator):\n",
        "    model.eval()\n",
        "\n",
        "    out = model(data)\n",
        "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
        "\n",
        "    train_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['train']],\n",
        "        'y_pred': y_pred[split_idx['train']],\n",
        "    })['acc']\n",
        "    valid_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['valid']],\n",
        "        'y_pred': y_pred[split_idx['valid']],\n",
        "    })['acc']\n",
        "    test_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['test']],\n",
        "        'y_pred': y_pred[split_idx['test']],\n",
        "    })['acc']\n",
        "\n",
        "    return train_acc, valid_acc, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bL825CiaOxym",
        "outputId": "ff3ee470-862e-45b5-b9a0-122188482d8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 0.08 GB: 100%|██████████| 81/81 [00:16<00:00,  4.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/arxiv.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 7695.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 1934.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "128\n",
            "Data(num_nodes=169343, x=[169343, 128], node_year=[169343, 1], y=[169343, 1], adj_t=[169343, 169343, nnz=1166243])\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "device = torch.device(device)\n",
        "#Here the data is already splited by the OGB\n",
        "dataset = PygNodePropPredDataset(name='ogbn-arxiv', transform=T.ToSparseTensor())\n",
        "print(dataset.num_node_features)\n",
        "#split data\n",
        "split_idx = dataset.get_idx_split()\n",
        "# data\n",
        "data = dataset[0]#data dans la position zero on a un seul dataset\n",
        "print(data)\n",
        "data = data.to(device)\n",
        "data.adj_t = data.adj_t.to_symmetric()#important pour enlever l'orientation\n",
        "\n",
        "# **********************************\n",
        "split_idx = dataset.get_idx_split() \n",
        "train_idx = split_idx['train'].to(device)\n",
        "evaluator = Evaluator(name='ogbn-arxiv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkHNXurEO2_O",
        "outputId": "fb11c098-b66c-44ad-9ce1-3707d8120fc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Loss: 4.0140, Train: 27.91%, Valid: 30.05%\n",
            "Epoch: 02, Loss: 2.5496, Train: 31.96%, Valid: 40.06%\n",
            "Epoch: 03, Loss: 2.2060, Train: 33.35%, Valid: 38.26%\n",
            "Epoch: 04, Loss: 2.0134, Train: 44.19%, Valid: 46.51%\n",
            "Epoch: 05, Loss: 1.8693, Train: 46.84%, Valid: 45.99%\n",
            "Epoch: 06, Loss: 1.7406, Train: 48.45%, Valid: 48.12%\n",
            "Epoch: 07, Loss: 1.6574, Train: 50.76%, Valid: 51.10%\n",
            "Epoch: 08, Loss: 1.5864, Train: 54.22%, Valid: 55.76%\n",
            "Epoch: 09, Loss: 1.5268, Train: 55.87%, Valid: 60.03%\n",
            "Epoch: 10, Loss: 1.4875, Train: 56.42%, Valid: 60.83%\n",
            "Epoch: 11, Loss: 1.4532, Train: 56.68%, Valid: 61.07%\n",
            "Epoch: 12, Loss: 1.4188, Train: 57.23%, Valid: 61.45%\n",
            "Epoch: 13, Loss: 1.3917, Train: 58.38%, Valid: 61.77%\n",
            "Epoch: 14, Loss: 1.3597, Train: 59.25%, Valid: 62.14%\n",
            "Epoch: 15, Loss: 1.3304, Train: 59.78%, Valid: 62.37%\n",
            "Epoch: 16, Loss: 1.3115, Train: 60.42%, Valid: 62.84%\n",
            "Epoch: 17, Loss: 1.2938, Train: 61.33%, Valid: 63.54%\n",
            "Epoch: 18, Loss: 1.2751, Train: 62.29%, Valid: 64.00%\n",
            "Epoch: 19, Loss: 1.2532, Train: 62.78%, Valid: 64.19%\n",
            "Epoch: 20, Loss: 1.2405, Train: 62.85%, Valid: 64.08%\n",
            "Epoch: 21, Loss: 1.2270, Train: 63.04%, Valid: 64.25%\n",
            "Epoch: 22, Loss: 1.2147, Train: 63.67%, Valid: 64.74%\n",
            "Epoch: 23, Loss: 1.2033, Train: 64.45%, Valid: 65.43%\n",
            "Epoch: 24, Loss: 1.1935, Train: 65.04%, Valid: 65.80%\n",
            "Epoch: 25, Loss: 1.1830, Train: 65.32%, Valid: 66.07%\n",
            "Epoch: 26, Loss: 1.1814, Train: 65.43%, Valid: 66.31%\n",
            "Epoch: 27, Loss: 1.1692, Train: 65.67%, Valid: 66.51%\n",
            "Epoch: 28, Loss: 1.1614, Train: 65.93%, Valid: 66.75%\n",
            "Epoch: 29, Loss: 1.1519, Train: 66.22%, Valid: 67.06%\n",
            "Epoch: 30, Loss: 1.1462, Train: 66.74%, Valid: 67.43%\n",
            "Epoch: 31, Loss: 1.1322, Train: 67.23%, Valid: 67.69%\n",
            "Epoch: 32, Loss: 1.1304, Train: 67.60%, Valid: 67.98%\n",
            "Epoch: 33, Loss: 1.1240, Train: 67.90%, Valid: 68.22%\n",
            "Epoch: 34, Loss: 1.1192, Train: 68.00%, Valid: 68.48%\n",
            "Epoch: 35, Loss: 1.1145, Train: 68.19%, Valid: 68.62%\n",
            "Epoch: 36, Loss: 1.1088, Train: 68.47%, Valid: 68.75%\n",
            "Epoch: 37, Loss: 1.1050, Train: 68.59%, Valid: 68.75%\n",
            "Epoch: 38, Loss: 1.0982, Train: 68.66%, Valid: 68.60%\n",
            "Epoch: 39, Loss: 1.0942, Train: 68.65%, Valid: 68.54%\n",
            "Epoch: 40, Loss: 1.0842, Train: 68.68%, Valid: 68.80%\n",
            "Epoch: 41, Loss: 1.0819, Train: 68.75%, Valid: 69.11%\n",
            "Epoch: 42, Loss: 1.0760, Train: 68.81%, Valid: 69.10%\n",
            "Epoch: 43, Loss: 1.0737, Train: 68.94%, Valid: 69.05%\n",
            "Epoch: 44, Loss: 1.0733, Train: 69.18%, Valid: 69.18%\n",
            "Epoch: 45, Loss: 1.0663, Train: 69.40%, Valid: 69.29%\n",
            "Epoch: 46, Loss: 1.0644, Train: 69.40%, Valid: 68.96%\n",
            "Epoch: 47, Loss: 1.0621, Train: 69.61%, Valid: 69.16%\n",
            "Epoch: 48, Loss: 1.0540, Train: 69.73%, Valid: 69.41%\n",
            "Epoch: 49, Loss: 1.0524, Train: 69.76%, Valid: 69.30%\n",
            "Epoch: 50, Loss: 1.0539, Train: 69.87%, Valid: 69.36%\n",
            " Test 0.6878587741497438\n"
          ]
        }
      ],
      "source": [
        "from torch._C import BenchmarkExecutionStats\n",
        "log_steps = 1\n",
        "Accvalid = []\n",
        "Acctest = []\n",
        "besttest = 0\n",
        "bestvalid = 0\n",
        "\n",
        "#Hyperparameters\n",
        "epochs = 50\n",
        "hidden_channels = 512\n",
        "dropout = 0.5\n",
        "lr = 0.01\n",
        "\n",
        "model = GCN(data, dataset.num_classes).to(device)\n",
        "model.reset_parameters()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(1, 1 + epochs):\n",
        "  loss = train(model, data, train_idx, optimizer)\n",
        "  result = test(model, data, split_idx, evaluator)\n",
        "\n",
        "  if epoch % log_steps == 0:\n",
        "    train_acc, valid_acc, test_acc = result\n",
        "    print(f'Epoch: {epoch:02d}, '\n",
        "          f'Loss: {loss:.4f}, '\n",
        "          f'Train: {100 * train_acc:.2f}%, '\n",
        "          f'Valid: {100 * valid_acc:.2f}%')\n",
        "                  \n",
        "  \n",
        "  \n",
        "  if (bestvalid<valid_acc) : \n",
        "      bestvalid=valid_acc\n",
        "      besttest=test_acc\n",
        "\n",
        "print(f' Test {besttest}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}